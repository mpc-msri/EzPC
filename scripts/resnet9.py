# -*- coding: utf-8 -*-
"""augmentaion&resnets.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/vaibhavhaswani/resnet9-cifar10-pt/blob/master/resnet9_cifar.ipynb
"""

# Commented out IPython magic to ensure Python compatibility.
import torch
import torch.nn as nn
import torchvision
import torch.nn.functional as F
from torchvision.datasets import ImageFolder
from torch.utils.data import random_split
from torch.utils.data import DataLoader
import torchvision.transforms as tt
from torchvision.utils import make_grid
import matplotlib.pyplot as plt
# %matplotlib inline

import tarfile
from torchvision.datasets.utils import download_url
# dataset_url = "https://s3.amazonaws.com/fast-ai-imageclas/cifar10.tgz"
# download_url(dataset_url, '.')

# Extract from archive
# with tarfile.open('./cifar10.tgz', 'r:gz') as tar:
#     tar.extractall(path='./datasets')

# data_dir='datasets/cifar10/'

"""We can create training and validation datasets using the `ImageFolder` class from `torchvision`. In addition to the `ToTensor` transform, we'll also apply some other transforms to the images. There are a few important changes we'll make while creating PyTorch datasets for training and validation:

1. **Use test set for validation**: Instead of setting aside a fraction (e.g. 10%) of the data from the training set for validation, we'll simply use the test set as our validation set. This just gives a little more data to train with. In general, once you have picked the best model architecture & hypeparameters using a fixed validation set, it is a good idea to retrain the same model on the entire dataset just to give it a small final boost in performance.
2. **Channel-wise data normalization**: We will normalize the image tensors by subtracting the mean and dividing by the standard deviation across each channel. As a result, the mean of the data across each channel is 0, and standard deviation is 1. Normalizing the data prevents the values from any one channel from disproportionately affecting the losses and gradients while training, simply by having a higher or wider range of values that others.

<img src="https://i.imgur.com/LYxXBVg.png" width="360">


3. **Randomized data augmentations**: We will apply randomly chosen transformations while loading images from the training dataset. Specifically, we will pad each image by 4 pixels, and then take a random crop of size 32 x 32 pixels, and then flip the image horizontally with a 50% probability. Since the transformation will be applied randomly and dynamically each time a particular image is loaded, the model sees slightly different images in each epoch of training, which allows it generalize better.

![data-augmentation](https://imgaug.readthedocs.io/en/latest/_images/cropandpad_percent.jpg)

### Data Augmentation and transformations
"""

#First we'll create a stat tuple with mean and std deviation of each channel pixel 
# stats=((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))   #((mean values of R,G,B),((std dev of R,G,B)))

# # Now we'll creating compositions for train and val transforms
# # tt.Compose() Composes several transforms together
# train_tfms=tt.Compose([
#     tt.RandomCrop(32,padding=4,padding_mode='reflect'), #Crop the given image at a random location , padding will padd the image borders with certain pixels before cropping and reflect method will specify that the padded pixels are reflection of pixels of image while zero padding will add black border around
#     tt.RandomHorizontalFlip(), #Horizontally flip the given image randomly with a given probability. default is .5
#     #tt.RandomRotation(40), #Rotate the image by given angle.
#     #tt.RandomResizedCrop(256, scale=(0.5,0.9), ratio=(1, 1)), #Crop the given image to random size and aspect ratio.
#     #tt.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1), #Randomly change the brightness, contrast and saturation of an image.
#     tt.ToTensor(),
#     tt.Normalize(*stats,inplace=True) #Normalize a tensor image with mean and standard deviation.
# ])
# #validation data does'nt require the transformations other than ToTensor and Normalization
# val_tfms=tt.Compose([
#     tt.ToTensor(),
#     tt.Normalize(*stats,inplace=True)
# ])

# # Getting validation (test) and train dataset with transformations
# train_ds=ImageFolder(data_dir+'train',transform=train_tfms)
# val_ds=ImageFolder(data_dir+'test',transform=val_tfms)

# #creating dataloaders
# batch_size=512
# train_dl=DataLoader(train_ds,batch_size,pin_memory=True,num_workers=4,shuffle=True)
# val_dl=DataLoader(val_ds,batch_size*2,pin_memory=True,num_workers=4)

# #showing a batch
# def denorm(images,mean,stds):
#     '''Denormalizes the pixels'''
#     mean=torch.tensor(mean).reshape(1,3,1,1)
#     stds=torch.tensor(stds).reshape(1,3,1,1)
#     return images*stds+mean

# def showbatch(dl):
#     for i,l in dl:
#         plt.figure(figsize=(12,10))
#         i=denorm(i,*stats)
#         plt.imshow(make_grid(i[:64],nrow=8).permute(1,2,0).clamp(0,1)) #clamp is used to keep tensors in min to max range so we specified 0to1
#         plt.axis('off')
#         plt.show();
#         break

# showbatch(train_dl)

# """**Gpu interfacing functions**"""

device=torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')

def to_device(data,device):
    if isinstance(data ,(list,tuple)):
        return [to_device(x,device) for x in data]
    return data.to(device)

# class DeviceDataLoader:
#     def __init__(self,dl,device):
#         self.dl=dl
#         self.device=device
#     def __iter__(self):
#         for b in self.dl:
#             yield to_device(b,self.device)
#     def __len__(self):
#         return len(self.dl)

# train_dl=DeviceDataLoader(train_dl,device)
# val_dl=DeviceDataLoader(val_dl,device)

"""# Residual Neural Network with Batch Normalization

One of the key changes to our CNN model this time is the addition of the resudial block, which adds the original input back to the output feature map obtained by passing the input through one or more convolutional layers.

![](https://miro.medium.com/max/1140/1*D0F3UitQ2l5Q0Ak-tjEdJg.png)

"""

class SimpleResudialBlock(nn.Module) :
    def __init__(self):
        super().__init__()
        self.conv1=nn.Conv2d(3,3,kernel_size=3,stride=1,padding=1)
        self.relu1=nn.ReLU()
        self.conv2=nn.Conv2d(3,3,kernel_size=3,stride=1,padding=1)
        self.relu2=nn.ReLU()
    def forward(self,x):
        out=self.conv1(x)
        out=self.relu1(out)
        out=self.conv2(out)
        return self.relu2(out) + x #we'll add the input after applying relu
#Note:for a residual block input channels and output channels of inside layers should be same

# simple_res= to_device(SimpleResudialBlock(),device)
# for img,_ in train_dl:
#     print(img.shape)
#     out=simple_res(img)
#     print(out.shape)
#     break

# del simple_res,img,_
# torch.cuda.empty_cache()

# #Base Training model
class BaseModel(nn.Module):
    def train_step(self,batch):
        img,label=batch
        out=self(img)
        loss=F.cross_entropy(out,label)
        return loss
    def val_step(self,batch):
        img,label=batch
        out=self(img)
        loss=F.cross_entropy(out,label)
        acc=accuracy(out,label)
        return {'val_loss':loss,'val_acc':acc}
    def val_epoch_end(self,outputs):
        batch_loss=[i['val_loss'] for i in outputs]
        batch_acc=[i['val_acc'] for i in outputs]
        epoch_loss=torch.stack(batch_loss).mean()
        epoch_acc=torch.stack(batch_acc).mean()
        return {'val_loss':epoch_loss.item(),'val_acc':epoch_acc.item()}
    
    def epoch_end(self, epoch, result):
        print(f"Epoch [{epoch}], train_loss: {result['train_loss']:.4f}, val_loss: {result['val_loss']:.4f}, val_acc: {result['val_acc']:.4f}")
        
# def accuracy(outputs, labels):
#     _, preds = torch.max(outputs, dim=1)
#     return torch.tensor(torch.sum(preds == labels).item() / len(preds))

"""This seeming small change produces a drastic improvement in the performance of the model. Also, after each convolutional layer, we'll add a batch normalization layer, which normalizes the outputs of the previous layer. 

We will use the ResNet9 architecture, as described in [this blog series](https://www.myrtle.ai/2018/09/24/how_to_train_your_resnet/) :

![resnet-9](https://github.com/lambdal/cifar10-fast/raw/master/net.svg?sanitize=true)
"""

def conv_block(in_ch,out_ch,pool=False):
    '''Represents single convolution block'''
    layers=[nn.Conv2d(in_ch,out_ch,kernel_size=3,stride=1,padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU()]
    if pool:
        layers.append(nn.MaxPool2d(2,2))
    return nn.Sequential(*layers)

class ResNet9(BaseModel):
    def __init__(self):
        super().__init__()
        #in 32*32*3
        self.conv1=conv_block(3,64)   #32*32*64
        self.conv2=conv_block(64,128,True) #16*16*128
        
        self.res1=nn.Sequential(conv_block(128,128),
                                conv_block(128,128))
        self.conv3=conv_block(128,256,True) #8*8*256
        self.conv4=conv_block(256,512,True) #4*4*512
        self.res2=nn.Sequential(conv_block(512,512),
                                conv_block(512,512))
        
        self.classifier=nn.Sequential(nn.MaxPool2d(4), #1*1*512
                                      nn.Flatten(),  #512
                                      nn.Dropout(0.2), #removing 20% of activations
                                      nn.Linear(512,10)
                                     ) 
    def forward(self,x):
        out=self.conv1(x)
        self.y = out
        out=self.conv2(out)
        out=self.res1(out)+out
        out=self.conv3(out)
        out=self.conv4(out)
        out=self.res2(out)+out
        out=self.classifier(out)
        return out

model=to_device(ResNet9(),device)
model.eval()
sd = torch.load('cifar10_resnet9-01.pth', map_location=torch.device('cpu'))
model.load_state_dict(sd)
# print(model.classifier[3].weight[0][0].item())
# print(model.conv1[0].weight[0][0][0][0])

# make an array filled with 1
a = torch.ones(1, 3, 32, 32)
model(a)
# print("weight =", model.conv1[1].weight[0])
# print("bias =", model.conv1[1].bias[0])
# print("running_mean =", model.conv1[1].running_mean[0])
# print("running_var =", model.conv1[1].running_var[0])
# print(model.y.size())
# print(model.y[0][0][0][0].item())

print(model(a))
