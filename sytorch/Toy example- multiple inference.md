## Multiple inference

Given an model onnx file, OnnxBridge can be used to generate an executable which can be run on two VMs, Server and Client (owning the model weights and input image respectively) and a Dealer (which pre-generates the randomness for the inference.), to get the secure inference output. To do this, use the `ezpc-cli-2.sh` script by running the following command locally (not neccesarily on a VM):

```bash
./ezpc-cli-2.sh -m /absolute/path/to/model.onnx -preprocess /absolute/path/to/preprocess.py -s server-ip -d dealer-ip
```

In the above command, the paths are not local, but are the locations on the respective VMs. That is, `/absolute/path/to/model.onnx` is the path of model.onnx file on the server VM, `/absolute/path/to/preprocess.py` is the path of preprocessing script on the server VM. To write the preprocessing script for your use case, refer to the preprocessing file of the [chexpert demo](/Athos/demos/onnx/pre_process.py). If your preprocessing script uses some additional python packages, make sure they are installed on the server and client VMs. Also, ensure that the client can communicate with the server through the IP address provided on the ports between the range 42002-42100. Optionally, you can also pass the following arguments:

- `-b <backend>`: the MPC backend to use (default: `LLAMA`)
- `-scale <scale>`: the scaling factor for the model input (default: `15`)
- `-bl <bitlength>`: the bitlength to use for the MPC computation (default: `40`)
- `-nt <numthreads>`: the number of threads to use for MPC computation (default: `4`)

The script generates 4 scripts:

- `server.sh` - Transfer this script to the server VM in any empty directory. Running this script (without any argument) reads the ONNX file, strips model weights out of it, dumps sytorch code, zips the code required to be sent to the client and dealer and waits for the client to download the zip. Once the zip is transfered, the script waits for dealer to generate the randomness and then starts the inference once the client connects. Once inference is complete, it downloads fresh randomness generated by dealer and again waits for client to start inference, this happens in a loop for multiple inference.
- `client-offline.sh` - Transfer this script to the client VM in any empty directory. Running this script fetches the stripped code from server and compiles the model. This script must be run on client VM parallely while server VM is running it's server script. 
- `client-online.sh` - It takes as input absolute path of image for inference. Transfer this script to the client VM in the same directory. Running this script downloads randomness from dealer,  preprocesses the input, connects with the server and starts the inference. After the secure inference is complete, inference output is printed and saved in `output.txt` file. This script needs to be run every time for a new inference with a new input.
- `dealer.sh` - Transfer this script to the dealer VM in any empty directory. Running this script waits for server to send the zip file, after which it generates and allows the client and server script to automatically download the co-related randomness for server and client. Once transferred, it generates a fresh pair of co-related randomness keys and again allows server and client to download it in a loop for multiple inference.

- Use 'clean' as `script.sh clean` with any of above script to clean the setup. This removes all files created by script from the current directory except the script itself. [Note: **This might remove all files from the current directory, keep backup of any important file.**]

## Toy example - LeNet-MNIST inference

Using the above instructions, we now demonstrate LeNet inference on MNIST images. We assume that we start at the home path `/home/<user>` on all machines. The below instructions also work on three terminals opened on a single machine (each terminal representing client, server and local computer) by passing `127.0.0.1` as IP address. 

1. On all machines, install dependencies.

```bash
sudo apt update
sudo apt install libeigen3-dev cmake build-essential git zip
```

2. On all machines, install the python dependencies in a virtual environment.

```bash
sudo apt install python3.8-venv
python3 -m venv venv
source venv/bin/activate
wget https://raw.githubusercontent.com/mpc-msri/EzPC/master/OnnxBridge/requirements.txt
pip install -r requirements.txt
pip install tqdm pyftpdlib
```
3. Download ONNX file and preprocessing script for LeNet on the server and make a temporary directory.

```bash
mkdir lenet-demo-server
cd lenet-demo-server
wget https://github.com/kanav99/models/raw/main/lenet.onnx
wget https://github.com/kanav99/models/raw/main/preprocess.py
mkdir tmp
cd tmp
```

4. Download the test image on the client and make a temporary directory.

```bash
mkdir lenet-demo-client
cd lenet-demo-client
wget https://github.com/kanav99/models/raw/main/input.jpg
mkdir tmp
cd tmp
```

5. Make a temporary directory for dealer.

```bash
mkdir lenet-demo-dealer
cd lenet-demo-dealer
mkdir tmp
cd tmp
```

6. On the local computer, clone EzPC repository, generate the scripts and transfer them to respective machines. If server, client and dealer are in same local network, then pass the local network IP in the `ezpc_cli-2.sh` command.

```bash
git clone https://github.com/mpc-msri/EzPC
cd EzPC/sytorch
chmod +x ezpc-cli-2.sh
./ezpc-cli-2.sh -m /home/<user>/lenet-demo-server/lenet.onnx -preprocess /home/<user>/lenet-demo-server/preprocess.py -s <SERVER-IP> -d <DEALER-IP>
scp server.sh <SERVER-IP>:/home/<user>/lenet-demo-server/tmp/
scp dealer.sh  <DEALER-IP>:/home/<user>/lenet-demo-dealer/tmp/
scp client-offline.sh <CLIENT-IP>:/home/<user>/lenet-demo-client/tmp/
scp client-online.sh  <CLIENT-IP>:/home/<user>/lenet-demo-client/tmp/
```

7. On all machines, make the bash scripts executable and execute them.

```bash
# (on server)
chmod +x server.sh
./server.sh

# (on dealer)
chmod +x dealer.sh
./dealer.sh

# (on client)
chmod +x client-offline.sh client-online.sh
./client-offline.sh
```

8. Once client-offline.sh script completes, run below script, as server waits for client to start inference. The inference logs get printed on the client terminal.

```bash
# (on client for every inference sequentially)
./client-online.sh /home/<user>/lenet-demo-client/input.jpg
# Run this script for every inference, after server has started downloading keys from dealer.
# server and dealer runs in loop to handle multiple inference with fresh co-related randomness.
```

In this particular example, you should get a score array of `[-2.71362 1.06747 4.43045 0.795044 -3.21173 -2.39871 -8.49094 10.3443 1.0567 -0.694458]`, which is maximum at index 7, which is indeed expected as the [input.jpg](https://github.com/kanav99/models/raw/main/input.jpg) file contains an image of handwritten 7.
