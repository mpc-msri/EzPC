## Inference App - LLAMA

Given an model onnx file, OnnxBridge can be used to generate an executable which can be run on two VMs, Server and Client (owning the model weights and input image respectively) and a Dealer (which pre-generates the randomness for the inference.), to get the secure inference output. Along with this we can use the Inference-App to give a GUI for inferencing. To generate the scripts involved in the Inference-App, use the `ezpc-cli-app.sh` script by running the following command locally (not neccesarily on a VM):

```bash
./ezpc-cli-app.sh -m /absolute/path/to/model.onnx -s server-ip -d dealer-ip [-nt num_threads]
```

In the above command, the paths are not local, but are the locations on the respective VMs. That is, `/absolute/path/to/model.onnx` is the path of model.onnx file on the server VM. <br/>
 We also have to write the preprocessing script for our use case, refer to the preprocessing file of the [chexpert demo](../frontend/Assets/preprocess.py). If your preprocessing script uses some additional python packages, make sure they are installed on the frontend VM. Also, ensure that the client can communicate with the server through the IP address provided on the ports between the range 42002-42100. Optionally, you can also pass the following arguments:

- `-scale <scale>`: the scaling factor for the model input (default: `15`)
- `-bl <bitlength>`: the bitlength to use for the MPC computation (default: `40`)
- `-nt <numthreads>`: the number of threads to use for MPC computation (default: `4`)

The script generates 4 scripts:

- `server.sh` - Transfer this script to the server VM in any empty directory. Running this script (without any argument) reads the ONNX file, strips model weights out of it, dumps sytorch code, zips the code required to be sent to the client and dealer and waits for the client to download the zip. Once the zip is transfered, the script waits for dealer to generate the randomness and then starts the inference once the client connects. Once inference is complete, it downloads fresh randomness generated by dealer and again waits for client to start inference, this happens in a loop for multiple inference.
- `client-offline.sh` - Transfer this script to the client VM in any empty directory. Running this script fetches the stripped code from server and compiles the model. This script must be run on client VM parallely while server VM is running it's server script. It downloads the keys from dealer, then starts a flask server to listen for inference requests from frontend on port 5000, where it receives a image as numpy array, initiates secure inference with server and returns the result to frontend and starts receiveing keys from dealer again.
- `client-online.sh` - It takes as input absolute path of numpy array of image for inference. Transfer this script to the client VM in the same directory. Running this script downloads randomness from dealer,  preprocesses the input, connects with the server and starts the inference. After the secure inference is complete, inference output is printed and saved in `output.txt` file. This script needs to be run every time for a new inference with a new input.
- `dealer.sh` - Transfer this script to the dealer VM in any empty directory. Running this script waits for server to send the zip file, after which it generates and allows the client and server script to automatically download the co-related randomness for server and client. Parallely, frontend also downloads masks from dealer.  Once transferred, it generates a fresh pair of co-related randomness keys and again allows server and client to download it in a loop for multiple inference. When Dealer is generating keys, either of Client/Server/Frontend are not allowed to download keys or mask.

- Use 'clean' as `script.sh clean` with any of above script to clean the setup. This removes all files created by script from the current directory except the script itself. [Note: **This might remove all files from the current directory, keep backup of any important file.**]